{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMYXgt1GpD2r",
        "outputId": "10a6a432-3cd3-4824-c118-a82b71631230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=705423b5d2b261e6bddbceb0d8484d0ae851fd00f1349254dd2ed97b4aff19fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qFf0cEOoMc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9507b1f-85f0-4c6e-ea92-07dda1f7250b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----+----+-----+-----+------+\n",
            "|  Voltas|Robert|Pedro|Rian|Renan|Bruce|Romulo|\n",
            "+--------+------+-----+----+-----+-----+------+\n",
            "| volta_1|    68|   87|  77|   65|   88|    65|\n",
            "| volta_2|    84|   85|  84|   70|   63|    90|\n",
            "| volta_3|    85|   68|  68|   74|   68|    61|\n",
            "| volta_4|    74|   89|  62|   87|   84|    74|\n",
            "| volta_5|    90|   84|  61|   82|   69|    85|\n",
            "| volta_6|    80|   61|  73|   66|   68|    90|\n",
            "| volta_7|    78|   76|  78|   81|   63|    61|\n",
            "| volta_8|    63|   73|  69|   65|   62|    70|\n",
            "| volta_9|    63|   87|  79|   66|   71|    75|\n",
            "|volta_10|    78|   72|  88|   89|   68|    88|\n",
            "+--------+------+-----+----+-----+-----+------+\n",
            "\n",
            "O campeão fez:  704  segundos, e seu nome é :  Bruce\n",
            "A classificação foi : \n",
            "+------+---------+\n",
            "|  nome|resultado|\n",
            "+------+---------+\n",
            "| Bruce|      704|\n",
            "|  Rian|      739|\n",
            "| Renan|      745|\n",
            "|Romulo|      759|\n",
            "|Robert|      763|\n",
            "| Pedro|      782|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "               .appName('SparkByExamples.com') \\\n",
        "               .getOrCreate()\n",
        "\n",
        "data = [(\"volta_1\", random.randint(60,90), random.randint(58,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_2\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_3\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_4\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_5\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_6\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_7\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_8\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_9\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90)),\n",
        "        (\"volta_10\", random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90), random.randint(60,90))]\n",
        "\n",
        "columns = [\"Voltas\", \"Robert\", \"Pedro\", \"Rian\", \"Renan\", \"Bruce\", \"Romulo\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.show()\n",
        "\n",
        "\n",
        "data_novo = [(\"Robert\",df.agg({'Robert' : 'sum'}).collect()[0][0]),\n",
        "             (\"Pedro\",df.agg({'Pedro' : 'sum'}).collect()[0][0]),\n",
        "             (\"Rian\",df.agg({'Rian' : 'sum'}).collect()[0][0]),\n",
        "             (\"Renan\",df.agg({'Renan' : 'sum'}).collect()[0][0]),\n",
        "             (\"Bruce\",df.agg({'Bruce' : 'sum'}).collect()[0][0]),\n",
        "             (\"Romulo\",df.agg({'Romulo' : 'sum'}).collect()[0][0])]\n",
        "\n",
        "\n",
        "nova_coluna = [\"nome\",\"resultado\"]\n",
        "\n",
        "df3 = spark.createDataFrame(data_novo, nova_coluna )\n",
        "#df3.show()\n",
        "#df3.orderBy(['resultado'], ascending = [True]).show()\n",
        "\n",
        "valor_campeao = df3.orderBy(['resultado'], ascending = [True]).collect()[0][1]\n",
        "campeao = df3.orderBy(['resultado'], ascending = [True]).collect()[0][0]\n",
        "\n",
        "print(\"O campeão fez: \", valor_campeao , \" segundos, e seu nome é : \", campeao )\n",
        "\n",
        "print(\"A classificação foi : \")\n",
        "df3.orderBy(['resultado'], ascending = [True]).show()"
      ]
    }
  ]
}